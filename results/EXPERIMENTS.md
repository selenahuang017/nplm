(â‰¤3/4 page: dataset, preprocessing choices, model sizes tried, what helped/hurt).

- Dataset: Brown
- Preprocessing choices: word level tokenizer (provided)
- Model sizes: small mostly
- Training parameters: default provided to start with.
    - I used ADAM as the optimizer though the paper uses SGD because they just used the standard at the time, which for us is ADAM (and should be better than SGD)
    - Due to my implementation, I had to use the given vocab size generated by the tokenizer. 
    - Because tiny.yml was expected to run within 10 minutes on a cpu, I shrunk a number of parameters, including the dimensions and vocabulary size. 
    - This was because my unfortunate computer was overheating attempting to do a vocab size of 20k.
    - For other parameters, I tended to choose along the lines of what the original paper did, so I didn't use dropout or clip_grad_norm. 

The large dimensions and batch sizes and vocab sizes really killed my machine. As I was testing and tuning, I found that decreasing the vocab size and the context size really increased the speed of training quite drastically, which makes sense because it definitely scales down the softmax and the dimensions in calculations. One interesting thing is that I wasn't sure if validation loss would keep decreasing, but it always did, likely because the nubmer of steps hadn't converged. If I had more processing power (or knew a better GPU) I would definitely attempt to run the full cycle until it converged.  