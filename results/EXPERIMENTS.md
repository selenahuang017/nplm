(â‰¤3/4 page: dataset, preprocessing choices, model sizes tried, what helped/hurt).

- Dataset: Brown
- Preprocessing choices: word level tokenizer (provided)
- Model sizes: small mostly
- Training parameters: default provided to start with.
    - I used ADAM as the optimizer though the paper uses SGD because they just used the standard at the time, which for us is ADAM (and should be better than SGD)
    - Due to my implementation, I had to use the given vocab size generated by the tokenizer. 
    - Because tiny.yml was expected to run within 10 minutes on a cpu, I shrunk a number of parameters:
        - vocab_size: 5000
        - embedding_dim: 128
        - hidden_dim: 128
        - batch_size: 256
        - max_steps: 2000
        - eval_every: 100
    - This was because my unfortunate computer was overheating attempting to do a vocab size of 20k.
    - For other parameters, I tended to choose along the lines of what the original paper did, so I didn't use dropout or clip_grad_norm. 

The large dimensions and batch sizes and vocab sizes really killed my machine haha....