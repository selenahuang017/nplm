seed: 1337
tokenizer_type: "word"   # or "bpe"
tokenizer_path: "artifacts/brown.json"
context_size: 5
vocab_size: 20000        # ignored if derived from tokenizer artifact
embedding_dim: 128
hidden_dim: 256
dropout: 0.1
optimizer: "adam"
lr: 0.001
batch_size: 256
max_steps: 20000
eval_every: 1000
clip_grad_norm: 1.0
device: "auto"   