## Ah yes, the LLM section

I used ChatGPT for this assignment. 

With much of the preprocessing, most of the LLM usage was fairly simple, just querying for syntax, such as how to save the nltk.corpus information, or how to pass in correct parameters for argument parsing, or how to save data as a jsonl. Some prompts I used include "can you download the nltk brown corpus into a directory" and "if argparse parameter is not a specific string return error". 

I did find that with simple questions, if the topics were diverse enough; ie I used the same chat log to ask everything from creating a python virtual environment to downloading and parsing information, suggestions at the end of each response always included something that tied back to previous questions, even if I moved on, or I was just asking theoretical questions. A lot of suggestions for next steps offered were completely not on track with what I wanted to do. For example, I had used it to understand data.py, and followed up with questions about DataLoader and other Torch related questions. But it kept asking "If you want, I can show a short snippet using your JSONL dataset and sliding window dataset showing how to inspect a batch and also access the underlying dataset. Do you want me to do that?" or something along those lines after it answered my questions. 

When creating the actual model (model.py) I relied heavily on ChatGPT to do the editing. Though I understand conceptually the different parts of creating a NN, I don't think I could write one from scratch. In the continuing chat where I had previously asked about the NPLM paper and any questions I had, like about the equations, I asked it to "implement the steps of the model described in the paper while following steps similar to in this article: https://medium.com/@dahami/a-neural-probabilistic-language-model-breaking-down-bengios-approach-4bf793a84426 What changes would there be?" and walked through the different steps it described to do the implementation. It provided something fairly similar to what is currently in model.py, though I made a couple edits to follow the formula given in the article. I also contrasted it with another implementation by someone else, and we walked through the differences and any changes needed. 

The following train.py and data.py were also written mostly by chatGPT, based on this article: https://naturale0.github.io/2021/02/04/Understanding-Neural-Probabilistic-Language-Model. I think I was a little confused on what exactly needed to be done in data.py, so I put the entire instruction set into the LLM: "If i should create a data.py that Represents JSONL dataset, on-the-fly windowing, used by training script as "dataloader" what is expected to be in data.py? train.py calls data.py with parameters, the JSONL dataset is the sharded training dataset for the above model.py" Question output: https://chatgpt.com/s/t_68d6e517008c81919099a180f8951c2e 

I was pretty impressed by how it illustrated all the different parts of what should be included in the file, and gave example output. It also explained how it would be integrated with train.py. After taking it, I changed some code stylistically and worked with fitting it into the system. Rarely did I paste straight code into the LLM, but it was very useful in clarifying questions. I will say that although I heard from friends in the industry that Claude was better at writing code, I did not see significant improvement between the two LLMs. In my experience (not specifically on this assignment) I have seen Claude hallucinate more, and create more lines of code that don't actually contribute to the output because the original code had a similar structure, but when it was changed, they forgot to update or remove unused variables. Although I caught ChatGPT doing something similar this time, it wasn't a major issue. 